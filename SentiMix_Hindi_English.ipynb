{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SentiMix-Hindi-English.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rql0Z4shGYv"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        " Name: Muhammad Saad\n",
        " CodaLab Username: Saad_33_B\n",
        " Rank: 30\n",
        "```\n",
        " Solution to codalab's SentiMix Hindi-English competition: \n",
        " [SentiMix Hindi-English]( https://competitions.codalab.org/competitions/20654)\n",
        "\n",
        "Train +Test Set After Cleaning:\n",
        " [Google Drive Link](https://drive.google.com/drive/folders/1hNnbsghCTVpsskq5KpHbTJSVJqOhubsZ?usp=sharing)\n",
        "\n",
        "Pretrained Model:\n",
        "[Google Drive Link](https://drive.google.com/file/d/1xmCegUpnMwCsoquCYYBqkxkKXf5m9wtA/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX3eaChSVfc4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxTcZ8XAVV-F"
      },
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVUDWonWmvH8"
      },
      "source": [
        "#Defining Paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88V4ZcTHblPB"
      },
      "source": [
        "#Defining Paths\n",
        "\n",
        "#Directories\n",
        "MOUNT_DIR = '/content/drive/'\n",
        "DRIVE_DIR = MOUNT_DIR+'MyDrive/'\n",
        "SENTIMIX_DIR = DRIVE_DIR+'sentimix-hindi/'\n",
        "\n",
        "#Train and Test Data Files\n",
        "TRAIN_SET_FILE = SENTIMIX_DIR + 'train_set_without_usernames.json'\n",
        "TEST_SET_FILE = SENTIMIX_DIR + 'test_set_without_usernames.json'\n",
        "\n",
        "#BERT Model \n",
        "MODEL_NAME = 'DeepPavlov/bert-base-multilingual-cased-sentence'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAYEmoHj77j7"
      },
      "source": [
        "# Connecting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeP1AkbGYj4m"
      },
      "source": [
        "#Connecting with google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(MOUNT_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcAIzJdfm5O2"
      },
      "source": [
        "# Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjlJ1zqkjkqk"
      },
      "source": [
        "import json\n",
        "\n",
        "#Loading our train dataset\n",
        "with open(TRAIN_SET_FILE) as file:\n",
        "  train_data = json.load(file)\n",
        "  \n",
        "#Loading our test dataset\n",
        "with open(TEST_SET_FILE) as file:\n",
        "  test_data = json.load(file)\n",
        "\n",
        "train_set = dict(train_data)\n",
        "test_set = dict(test_data)\n",
        "del train_data\n",
        "del test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kem9adJ8lxZc"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(train_set).T\n",
        "test_df = pd.DataFrame(test_set).T\n",
        "\n",
        "train_df['sentence'] = train_df['sentence'].apply(lambda sen: \" \".join(sen).strip())\n",
        "test_df['sentence'] = test_df['sentence'].apply(lambda sen: \" \".join(sen).strip())  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8O7bMcyORIa"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esHhSL-xhKs7"
      },
      "source": [
        "#Exploring our data\n",
        "train_df['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz00J6zxh4r7"
      },
      "source": [
        "#Encoding sentiments using integers\n",
        "labels = dict()\n",
        "for idx, lab in enumerate(train_df.sentiment.unique()):\n",
        "  labels[lab] = idx\n",
        "\n",
        "train_df['sentiment'] = train_df.sentiment.replace(labels)\n",
        "test_df['sentiment'] = test_df.sentiment.replace(labels)\n",
        "\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVRq0MXar63I"
      },
      "source": [
        "#Loading Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm-IeswNr5zt"
      },
      "source": [
        "#Tokenizer \n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "\n",
        "#For Training\n",
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    train_df.sentence.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=128, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "\n",
        "labels_train = torch.tensor(train_df.sentiment.values)\n",
        "\n",
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
        "\n",
        "#For Validation\n",
        "\n",
        "encoded_data_val = tokenizer.batch_encode_plus(\n",
        "    test_df.sentence.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True,\n",
        "    max_length=128, \n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "input_ids_val = encoded_data_val['input_ids']\n",
        "\n",
        "attention_masks_val = encoded_data_val['attention_mask']\n",
        "\n",
        "labels_val = torch.tensor(test_df.sentiment.values)\n",
        "\n",
        "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmT8TbKMteOo"
      },
      "source": [
        "# Loading Pretrained BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqQZHvlMp3Eb"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME,\n",
        "                                                      num_labels=len(labels),\n",
        "                                                      output_attentions=True,\n",
        "                                                      output_hidden_states=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CmipQxxX_Jx"
      },
      "source": [
        "#Creating Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TVk1vy_ryNd"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32 \n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, \n",
        "                              sampler=RandomSampler(dataset_train), \n",
        "                              batch_size=batch_size)\n",
        "\n",
        "dataloader_validation = DataLoader(dataset_val, \n",
        "                                   sampler=SequentialSampler(dataset_val), \n",
        "                                   batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig0lBuD1X33p"
      },
      "source": [
        "#Creating Optimizers and Schedulers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOZUn01wtyVK"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup, Adafactor\n",
        "                  \n",
        "optimizer = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=1e-5,\n",
        "    eps=(1e-30, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.8,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=int(0.125*len(dataloader_train)),\n",
        "                                            num_training_steps=len(dataloader_train)*epochs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbLhaR9ht9-z"
      },
      "source": [
        "# Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbCuWnZ6t5jj"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "def accuracy_per_class(preds, true_labels):\n",
        "\n",
        "    label_dict_inverse = {v: k for k, v in labels.items()}\n",
        "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = true_labels.flatten()\n",
        "\n",
        "    for label in np.unique(labels_flat):\n",
        "        y_preds = preds_flat[labels_flat==label]\n",
        "        y_true = labels_flat[labels_flat==label]\n",
        "        print(f'Class: {label_dict_inverse[label]}')\n",
        "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFYHuimXpwmq"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvXiAh80YMYJ"
      },
      "source": [
        "def evaluate(dataloader_val,model):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    for batch in dataloader_val:\n",
        "        \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    \n",
        "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "            \n",
        "    return loss_val_avg, predictions, true_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGyGy4MMuF4-"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 33\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "####\n",
        "# Uncomment the line below to load a previous saved model\n",
        "#model.load_state_dict(torch.load(f'{SENTIMIX_DIR}sentimix_model.model', map_location=torch.device(device)))\n",
        "###\n",
        "model.to(device)\n",
        "\n",
        "    \n",
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_train_total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        model.zero_grad()\n",
        "        \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }       \n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "         \n",
        "        \n",
        "    torch.save(model.state_dict(), f'{SENTIMIX_DIR}augmented_sentimix_BERT_epoch_{epoch}.model')\n",
        "        \n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "    \n",
        "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "    \n",
        "    val_loss, predictions, true_vals = evaluate(dataloader_validation,model)\n",
        "    val_f1 = f1_score_func(predictions, true_vals)\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\n",
        "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC4bgYpVlYdH"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaQLfabMu6Qs",
        "outputId": "ebfbca3c-a493-4b4a-f927-fbcb6c4160d4"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_eval = BertForSequenceClassification.from_pretrained(MODEL_NAME,\n",
        "                                                      num_labels=len(labels),\n",
        "                                                      output_attentions=False,\n",
        "                                                      output_hidden_states=False)\n",
        "\n",
        "model_eval.to(device)\n",
        "\n",
        "model_eval.load_state_dict(torch.load(f'{SENTIMIX_DIR}sentimix_model.model', map_location=torch.device('cpu')))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/bert-base-multilingual-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rwONGajnQco"
      },
      "source": [
        "## Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-dF_fQqFSlj",
        "outputId": "78886f33-d2d8-44c2-e475-91e674b8c466"
      },
      "source": [
        "_, predictions, true_vals = evaluate(dataloader_validation,model_eval)\n",
        "accuracy_per_class(predictions, true_vals)\n",
        "print(f'F1 Score: {f1_score_func(predictions, true_vals)}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class: negative\n",
            "Accuracy: 685/900\n",
            "\n",
            "Class: positive\n",
            "Accuracy: 813/1000\n",
            "\n",
            "Class: neutral\n",
            "Accuracy: 637/1100\n",
            "\n",
            "F1 Score: 0.7085907587775198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFq3fgqg7rrg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWH2turonWC7",
        "outputId": "68e18424-df69-4c7a-fe42-e8059418b3c3"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(f'Labels: {labels}')\n",
        "print(classification_report(true_vals,np.argmax(predictions, axis=1).flatten()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels: {'negative': 0, 'positive': 1, 'neutral': 2}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.76      0.73       900\n",
            "           1       0.77      0.81      0.79      1000\n",
            "           2       0.66      0.58      0.62      1100\n",
            "\n",
            "    accuracy                           0.71      3000\n",
            "   macro avg       0.71      0.72      0.71      3000\n",
            "weighted avg       0.71      0.71      0.71      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTU_C_kglov_"
      },
      "source": [
        "## Creating answers.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj11-jyIVlCe"
      },
      "source": [
        "with open(TEST_SET_FILE) as file:\n",
        "  test_dataset = json.load(file)\n",
        "\n",
        "test_dataset = dict(test_dataset)\n",
        "\n",
        "keys_list = [key for key in test_dataset.keys()]\n",
        "pred_list = np.argmax(predictions, axis=1).flatten()\n",
        "label_dict_inverse = {v: k for k, v in labels.items()}\n",
        "\n",
        "with open(f'{SENTIMIX_DIR}answer.txt','w') as file:\n",
        "    file.write('Uid,Sentiment\\n')\n",
        "    for key,pred in zip(keys_list,pred_list):\n",
        "        file.write(f'{key},{label_dict_inverse[pred]}\\n')\n",
        "\n",
        "del test_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RQ8-220s3Ty"
      },
      "source": [
        "# Credits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYvI9lKfkVsc"
      },
      "source": [
        "\n",
        "[Multi Class Text Classification With Deep Learning Using BERT](https://towardsdatascience.com/multi-class-text-classification-with-deep-learning-using-bert-b59ca2f5c613)\n"
      ]
    }
  ]
}